"""
–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–µ–≤–∞
"""
import asyncio
import logging
import random
from typing import Dict, Optional, TYPE_CHECKING
from datetime import datetime

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger

from app.config import config
from app.core.db import db_manager
from app.core.warmer import warmer
from app.core.reports import report_generator
from app.utils.url_grouper import url_grouper
from app.utils.sitemap import sitemap_parser

if TYPE_CHECKING:
    from aiogram import Bot

logger = logging.getLogger(__name__)


class WarmingScheduler:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–µ–≤–∞"""
    
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.job_map: Dict[int, str] = {}  # domain_id -> apscheduler_job_id
        self.bot: Optional['Bot'] = None
    
    def set_bot(self, bot: 'Bot') -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –±–æ—Ç–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        self.bot = bot
        logger.info("Bot instance set for scheduler")
    
    def start(self) -> None:
        """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.scheduler.start()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ (–≤ 6:00 UTC = 9:00 UTC+3 –ú–∏–Ω—Å–∫)
        self.scheduler.add_job(
            self.send_daily_reports_task,
            trigger='cron',
            hour=6,
            minute=0,
            id='daily_reports',
            replace_existing=True
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è URL (–≤ 3:00 UTC = 6:00 UTC+3 –ú–∏–Ω—Å–∫ - –Ω–æ—á—å—é)
        self.scheduler.add_job(
            self.update_domains_urls_task,
            trigger='cron',
            hour=3,
            minute=0,
            id='update_urls',
            replace_existing=True
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –±—ç–∫–∞–ø–∞ –∫–∞–∂–¥—ã–π —á–∞—Å
        self.scheduler.add_job(
            self.auto_backup_task,
            trigger='interval',
            hours=1,
            id='auto_backup',
            replace_existing=True
        )
        
        logger.info("Scheduler started with daily reports at 06:00 UTC, URL updates at 03:00 UTC, and hourly backups")

    
    def shutdown(self) -> None:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        self.scheduler.shutdown()
        logger.info("Scheduler stopped")
    
    def parse_schedule(self, schedule: str) -> Optional[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        –§–æ—Ä–º–∞—Ç: "5m", "1h", "30m" –∏ —Ç.–¥.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è IntervalTrigger
        """
        if not schedule:
            return None
        
        schedule = schedule.strip().lower()
        
        try:
            if schedule.endswith('m'):
                minutes = int(schedule[:-1])
                return {"minutes": minutes}
            elif schedule.endswith('h'):
                hours = int(schedule[:-1])
                return {"hours": hours}
            elif schedule.endswith('s'):
                seconds = int(schedule[:-1])
                return {"seconds": seconds}
            else:
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –º–∏–Ω—É—Ç—ã
                minutes = int(schedule)
                return {"minutes": minutes}
        except ValueError:
            logger.error(f"Invalid schedule format: {schedule}")
            return None
    
    async def warm_domain_task(self, domain_id: int, job_id: int) -> None:
        """–ó–∞–¥–∞—á–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ –¥–æ–º–µ–Ω–∞"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ .env)
            # –ß—Ç–æ–±—ã –Ω–µ –≤—Å–µ –¥–æ–º–µ–Ω—ã –ø—Ä–æ–≥—Ä–µ–≤–∞–ª–∏—Å—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–∞ SaaS –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö
            if config.WARMER_DOMAIN_DELAY_MAX > 0:
                delay = random.uniform(config.WARMER_DOMAIN_DELAY_MIN, config.WARMER_DOMAIN_DELAY_MAX)
                logger.info(f"‚è∞ Scheduled warming task for domain_id={domain_id}, waiting {delay:.1f}s to avoid platform overload")
                await asyncio.sleep(delay)
            else:
                logger.info(f"‚è∞ Scheduled warming task for domain_id={domain_id} (no delay)")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω —Å URL
            domain = await db_manager.get_domain_by_id(domain_id)
            
            if not domain or not domain.is_active:
                logger.warning(f"Domain {domain_id} not found or inactive, removing job")
                self.remove_job(domain_id)
                return
            
            if not domain.urls:
                logger.warning(f"No URLs for domain {domain_id}")
                return
            
            # –ü–æ–ª—É—á–∞–µ–º Job –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã
            result = await db_manager.get_active_jobs()
            current_job = next((j for j in result if j.id == job_id), None)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä—É–ø–ø—É –∏–∑ Job (–¥–ª—è –∞–≤—Ç–æ–ø—Ä–æ–≥—Ä–µ–≤–∞)
            active_group = current_job.active_url_group if current_job else 3
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º URL –ø–æ –≥—Ä—É–ø–ø–µ –∏–∑ Job
            all_urls = [url.url for url in domain.urls]
            urls = url_grouper.filter_urls_by_group(all_urls, domain.name, active_group)
            
            logger.info(f"Scheduled warming for {domain.name} (group {active_group}): {len(urls)}/{len(all_urls)} URLs")
            
            # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º (–ø–µ—Ä–µ–¥–∞–µ–º –∏–º—è –¥–æ–º–µ–Ω–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            stats = await warmer.warm_site(urls, domain_name=domain.name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥—Ä–µ–≤–∞ –≤ –ë–î
            try:
                await db_manager.save_warming_result(
                    domain_id=domain_id,
                    started_at=stats["started_at"],
                    completed_at=stats["completed_at"],
                    total_requests=stats["total_requests"],
                    successful_requests=stats["success"],
                    failed_requests=stats["error"],
                    timeout_requests=stats["timeout"],
                    avg_response_time=stats["avg_time"],
                    min_response_time=stats["min_time"],
                    max_response_time=stats["max_time"],
                    warming_type="scheduled"
                )
                logger.info(f"üíæ Saved warming result to database for {domain.name}")
            except Exception as e:
                logger.error(f"Error saving warming result to DB: {e}", exc_info=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
            await db_manager.update_job_last_run(job_id)
            
            logger.info(f"‚úÖ Scheduled warming completed for {domain.name}: {stats}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)
            if config.SEND_WARMING_NOTIFICATIONS and self.bot:
                await self._send_warming_notification(domain, stats)
            
        except Exception as e:
            logger.error(f"Error in scheduled warming task for domain {domain_id}: {e}", exc_info=True)
    
    async def _send_warming_notification(self, domain, stats: Dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ–≤–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω—ã –ª–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        if not config.SEND_WARMING_NOTIFICATIONS:
            logger.debug(f"Warming notifications disabled, skipping for domain {domain.name}")
            return
        
        try:
            success_rate = (stats["success"] / stats["total_requests"] * 100) if stats["total_requests"] > 0 else 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–º–æ–¥–∑–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            if success_rate >= 90:
                status_emoji = "‚úÖ"
            elif success_rate >= 70:
                status_emoji = "‚ö†Ô∏è"
            else:
                status_emoji = "‚ùå"
            
            # 3. –£–±—Ä–∞—Ç—å "–æ–±—â–µ–µ –≤—Ä–µ–º—è" –∏–∑ –æ—Ç—á–µ—Ç–∞
            message = (
                f"{status_emoji} <b>–ê–≤—Ç–æ–ø—Ä–æ–≥—Ä–µ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω</b>\n\n"
                f"üåê –î–æ–º–µ–Ω: <b>{domain.name}</b>\n"
                f"üïí –í—Ä–µ–º—è: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\n\n"
                f"üìä <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</b>\n"
                f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: <b>{stats['total_requests']}</b>\n"
                f"‚Ä¢ ‚úÖ –£—Å–ø–µ—à–Ω–æ: <b>{stats['success']}</b> ({success_rate:.1f}%)\n"
                f"‚Ä¢ ‚è± –¢–∞–π–º–∞—É—Ç—ã: <b>{stats['timeout']}</b>\n"
                f"‚Ä¢ ‚ùå –û—à–∏–±–∫–∏: <b>{stats['error']}</b>\n"
                f"‚Ä¢ ‚è± –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: <b>{stats['avg_time']:.2f}s</b>"
            )
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–∞–Ω–∞–ª - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç—É–¥–∞
            if config.TECHNICAL_CHANNEL_ID:
                try:
                    logger.debug(f"Attempting to send notification to channel: {config.TECHNICAL_CHANNEL_ID}")
                    await self.bot.send_message(
                        chat_id=config.TECHNICAL_CHANNEL_ID,
                        text=message,
                        parse_mode="HTML"
                    )
                    logger.info(f"üì§ Notification sent to technical channel ({config.TECHNICAL_CHANNEL_ID}) for domain {domain.name}")
                except Exception as e:
                    logger.error(
                        f"‚ùå Failed to send notification to technical channel ({config.TECHNICAL_CHANNEL_ID}): {type(e).__name__}: {e}\n"
                        f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:\n"
                        f"1. –ë–æ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–∞–Ω–∞–ª –∫–∞–∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä\n"
                        f"2. –£ –±–æ—Ç–∞ –µ—Å—Ç—å –ø—Ä–∞–≤–æ '–ü—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è'\n"
                        f"3. ID –∫–∞–Ω–∞–ª–∞ —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å -100)",
                        exc_info=True
                    )
            else:
                # –ò–Ω–∞—á–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
                admins = await db_manager.get_all_admins()
                
                sent_count = 0
                for admin in admins:
                    try:
                        await self.bot.send_message(
                            chat_id=admin.id,
                            text=message,
                            parse_mode="HTML"
                        )
                        sent_count += 1
                    except Exception as e:
                        logger.warning(f"Failed to send notification to admin {admin.id}: {e}")
                
                logger.info(f"üì§ Notification sent to {sent_count}/{len(admins)} admins for domain {domain.name}")
            
        except Exception as e:
            logger.error(f"Error sending notifications: {e}", exc_info=True)
    
    def add_job(self, domain_id: int, job_id: int, schedule: str, start_delay: int = 0) -> bool:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        
        Args:
            domain_id: ID –¥–æ–º–µ–Ω–∞
            job_id: ID –∑–∞–¥–∞—á–∏
            schedule: –†–∞—Å–ø–∏—Å–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "10m")
            start_delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–¥–ª—è —É–º–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
        """
        try:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∑–∞–¥–∞—á—É, –µ—Å–ª–∏ –µ—Å—Ç—å
            self.remove_job(domain_id)
            
            # –ü–∞—Ä—Å–∏–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ
            interval_params = self.parse_schedule(schedule)
            
            if not interval_params:
                logger.error(f"Failed to parse schedule: {schedule}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∏–≥–≥–µ—Ä
            trigger = IntervalTrigger(**interval_params)
            
            # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, –ø–ª–∞–Ω–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            import datetime as dt
            if start_delay > 0:
                start_date = datetime.now() + dt.timedelta(seconds=start_delay)
                apscheduler_job = self.scheduler.add_job(
                    self.warm_domain_task,
                    trigger=trigger,
                    args=[domain_id, job_id],
                    id=f"warm_domain_{domain_id}",
                    replace_existing=True,
                    next_run_time=start_date  # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ start_delay —Å–µ–∫—É–Ω–¥
                )
                logger.info(f"‚úÖ Added scheduled job for domain {domain_id}: {schedule} (starts in {start_delay}s)")
            else:
                # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ –∑–∞–¥–µ—Ä–∂–∫–∏
                apscheduler_job = self.scheduler.add_job(
                    self.warm_domain_task,
                    trigger=trigger,
                    args=[domain_id, job_id],
                    id=f"warm_domain_{domain_id}",
                    replace_existing=True,
                )
                logger.info(f"‚úÖ Added scheduled job for domain {domain_id}: {schedule}")
            
            self.job_map[domain_id] = apscheduler_job.id
            return True
            
        except Exception as e:
            logger.error(f"Error adding job for domain {domain_id}: {e}", exc_info=True)
            return False
    
    def remove_job(self, domain_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∏–∑ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        try:
            if domain_id in self.job_map:
                job_id = self.job_map[domain_id]
                self.scheduler.remove_job(job_id)
                del self.job_map[domain_id]
                logger.info(f"‚úÖ Removed scheduled job for domain {domain_id}")
                return True
            return False
        except Exception as e:
            logger.error(f"Error removing job for domain {domain_id}: {e}")
            return False
    
    async def reload_jobs(self) -> None:
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –∏–∑ –±–∞–∑—ã —Å —É–º–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
        –î–æ–º–µ–Ω—ã –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞
        """
        logger.info("Reloading scheduled jobs from database...")
        
        try:
            active_jobs = await db_manager.get_active_jobs()
            
            if not active_jobs:
                logger.info("No active jobs to reload")
                return
            
            # –û—á–∏—â–∞–µ–º –≤—Å–µ —Ç–µ–∫—É—â–∏–µ –∑–∞–¥–∞—á–∏
            for domain_id in list(self.job_map.keys()):
                self.remove_job(domain_id)
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω—ã —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º URL –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            domains_info = []
            for job in active_jobs:
                if job.schedule:
                    domain = await db_manager.get_domain_by_id(job.domain_id)
                    if domain:
                        url_count = len(domain.urls) if domain.urls else 0
                        domains_info.append({
                            'job': job,
                            'domain_id': job.domain_id,
                            'domain_name': domain.name,
                            'url_count': url_count
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–æ–º–µ–Ω—ã, –ø–æ—Ç–æ–º –±–æ–ª—å—à–∏–µ
            domains_info.sort(key=lambda x: x['url_count'])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–º–µ–Ω—ã –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É
            total_domains = len(domains_info)
            
            logger.info(f"üìä Scheduling {total_domains} domains by size:")
            for i, info in enumerate(domains_info):
                # –ó–∞–¥–µ—Ä–∂–∫–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–º–µ–Ω–∞
                # –ú–∞–ª–µ–Ω—å–∫–∏–µ –¥–æ–º–µ–Ω—ã (0-50 URL) - —Å—Ç–∞—Ä—Ç—É—é—Ç –±—ã—Å—Ç—Ä–µ–µ
                # –°—Ä–µ–¥–Ω–∏–µ –¥–æ–º–µ–Ω—ã (50-200 URL) - —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞
                # –ë–æ–ª—å—à–∏–µ –¥–æ–º–µ–Ω—ã (>200 URL) - –±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                
                url_count = info['url_count']
                
                # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –æ—á–µ—Ä–µ–¥–∏
                base_delay = i * 30  # 30 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –∫–∞–∂–¥—ã–º –¥–æ–º–µ–Ω–æ–º
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞
                if url_count > 500:
                    size_delay = 120  # –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π –¥–æ–º–µ–Ω - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ 2 –º–∏–Ω—É—Ç—ã
                elif url_count > 200:
                    size_delay = 60   # –ë–æ–ª—å—à–æ–π –¥–æ–º–µ–Ω - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ 60 —Å–µ–∫
                elif url_count > 50:
                    size_delay = 30   # –°—Ä–µ–¥–Ω–∏–π –¥–æ–º–µ–Ω - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ 30 —Å–µ–∫
                else:
                    size_delay = 15   # –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–æ–º–µ–Ω - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ 15 —Å–µ–∫
                
                total_delay = base_delay + size_delay
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É —Å —É—á–µ—Ç–æ–º —Å—Ç–∞—Ä—Ç–æ–≤–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏
                job = info['job']
                self.add_job(job.domain_id, job.id, job.schedule, start_delay=total_delay)
                
                logger.info(
                    f"  {i+1}. {info['domain_name']}: {url_count} URLs ‚Üí "
                    f"start in {total_delay}s (base: {base_delay}s + size: {size_delay}s)"
                )
            
            logger.info(f"‚úÖ Reloaded {len(domains_info)} scheduled jobs with smart distribution")
            
        except Exception as e:
            logger.error(f"Error reloading jobs: {e}", exc_info=True)
    
    async def send_daily_reports_task(self) -> None:
        """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤"""
        if not self.bot:
            logger.warning("Bot instance not set, skipping daily reports")
            return
        
        logger.info("Sending daily reports...")
        await report_generator.send_daily_reports(self.bot)
    
    async def update_domains_urls_task(self) -> None:
        """–ó–∞–¥–∞—á–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è URL –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤"""
        logger.info("üîÑ Starting automatic URL update for all domains...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –¥–æ–º–µ–Ω—ã
            domains = await db_manager.get_all_domains()
            
            if not domains:
                logger.info("No domains to update")
                return
            
            updated_count = 0
            errors_count = 0
            
            for domain in domains:
                try:
                    logger.info(f"Updating URLs for domain: {domain.name}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ URL
                    new_urls = await sitemap_parser.discover_urls(domain.name)
                    
                    if not new_urls:
                        logger.warning(f"No URLs found for {domain.name}, skipping update")
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ä—ã–µ URL
                    old_urls = set(url.url for url in domain.urls)
                    new_urls_set = set(new_urls)
                    
                    logger.info(f"{domain.name}: Found {len(new_urls_set)} URLs (was {len(old_urls)})")
                    
                    # –ó–ê–©–ò–¢–ê: –ï—Å–ª–∏ –Ω–æ–≤—ã—Ö URL < 50% –æ—Ç —Å—Ç–∞—Ä—ã—Ö - —ç—Ç–æ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞, –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º
                    if old_urls and len(new_urls_set) < len(old_urls) * 0.5:
                        logger.error(
                            f"‚ùå Suspicious URL drop for {domain.name}: "
                            f"{len(old_urls)} ‚Üí {len(new_urls_set)} (>50% loss). "
                            f"Skipping update to prevent data loss."
                        )
                        if self.bot:
                            admins = await db_manager.get_all_admins()
                            error_msg = (
                                f"‚ö†Ô∏è <b>–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è URL</b>\n\n"
                                f"üåê –î–æ–º–µ–Ω: <b>{domain.name}</b>\n"
                                f"–ë—ã–ª–æ: {len(old_urls)} URL\n"
                                f"–ù–∞–π–¥–µ–Ω–æ: {len(new_urls_set)} URL\n\n"
                                f"–ü–æ—Ç–µ—Ä—è >50% URL - –≤–æ–∑–º–æ–∂–Ω–∞ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞.\n"
                                f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
                            )
                            for admin in admins:
                                try:
                                    await self.bot.send_message(admin.id, error_msg, parse_mode="HTML")
                                except: pass
                        continue
                    
                    # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤—ã–µ URL (–∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ)
                    added_urls = new_urls_set - old_urls
                    # –ù–∞—Ö–æ–¥–∏–º —É–¥–∞–ª–µ–Ω–Ω—ã–µ URL (–∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏, –Ω–æ –±–æ–ª—å—à–µ –Ω–µ—Ç)
                    removed_urls = old_urls - new_urls_set
                    
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ URL
                    if removed_urls:
                        await db_manager.delete_urls_by_domain(domain.id, list(removed_urls))
                        logger.info(f"Removed {len(removed_urls)} URLs from {domain.name}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ URL
                    if added_urls:
                        await db_manager.add_urls_to_domain(domain.id, list(added_urls))
                        logger.info(f"Added {len(added_urls)} new URLs to {domain.name}")
                    
                    updated_count += 1
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∞–º –µ—Å–ª–∏ –±—ã–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                    if self.bot and (len(added_urls) > 10 or len(removed_urls) > 10):
                        admins = await db_manager.get_all_admins()
                        
                        message = (
                            f"üìä <b>–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ URL</b>\n\n"
                            f"üåê –î–æ–º–µ–Ω: <b>{domain.name}</b>\n"
                            f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ: <b>{len(added_urls)}</b> URL\n"
                            f"‚ûñ –£–¥–∞–ª–µ–Ω–æ: <b>{len(removed_urls)}</b> URL\n"
                            f"üìÑ –í—Å–µ–≥–æ: <b>{len(new_urls_set)}</b> URL"
                        )
                        
                        for admin in admins:
                            try:
                                await self.bot.send_message(admin.id, message, parse_mode="HTML")
                            except Exception as e:
                                logger.warning(f"Failed to send update notification to admin {admin.id}: {e}")
                    
                except Exception as e:
                    logger.error(f"Error updating URLs for domain {domain.name}: {e}", exc_info=True)
                    errors_count += 1
            
            logger.info(f"‚úÖ URL update completed: {updated_count} domains updated, {errors_count} errors")
            
        except Exception as e:
            logger.error(f"Error in URL update task: {e}", exc_info=True)
    
    async def auto_backup_task(self) -> None:
        """–ó–∞–¥–∞—á–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –±—ç–∫–∞–ø–∞ –ë–î –∫–∞–∂–¥—ã–π —á–∞—Å"""
        if not self.bot:
            logger.warning("Bot instance not set, skipping backup task")
            return
        
        logger.info("üíæ Starting automatic backup...")
        
        try:
            import subprocess
            from pathlib import Path
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—ç–∫–∞–ø–æ–≤
            backup_dir = Path("/app/backups")
            backup_dir.mkdir(exist_ok=True, parents=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±—ç–∫–∞–ø–∞
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"siteheater_backup_{timestamp}.sql.gz.enc"
            backup_path = backup_dir / backup_filename
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –±—ç–∫–∞–ø —á–µ—Ä–µ–∑ docker
            result = subprocess.run(
                [
                    "docker", "exec", "siteheater_postgres",
                    "pg_dump", "-U", config.POSTGRES_USER, "-d", config.POSTGRES_DB
                ],
                capture_output=True,
                text=True,
                timeout=300  # 5 –º–∏–Ω—É—Ç
            )
            
            if result.returncode != 0:
                raise Exception(f"pg_dump failed: {result.stderr}")
            
            # –°–∂–∏–º–∞–µ–º –∏ —à–∏—Ñ—Ä—É–µ–º –±—ç–∫–∞–ø
            import gzip
            
            # –°–Ω–∞—á–∞–ª–∞ —Å–∂–∏–º–∞–µ–º
            compressed_path = backup_dir / f"siteheater_backup_{timestamp}.sql.gz"
            with gzip.open(compressed_path, 'wb') as f:
                f.write(result.stdout.encode('utf-8'))
            
            # –ó–∞—Ç–µ–º —à–∏—Ñ—Ä—É–µ–º (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω –ø–∞—Ä–æ–ª—å)
            if config.BACKUP_ENCRYPTION_PASSWORD:
                subprocess.run(
                    [
                        "openssl", "enc", "-aes-256-cbc",
                        "-salt", "-pbkdf2",
                        "-in", str(compressed_path),
                        "-out", str(backup_path),
                        "-pass", f"pass:{config.BACKUP_ENCRYPTION_PASSWORD}"
                    ],
                    check=True,
                    timeout=60
                )
                # –£–¥–∞–ª—è–µ–º –Ω–µ—Å–∂–∞—Ç—ã–π —Ñ–∞–π–ª
                compressed_path.unlink()
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—è, –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
                backup_path = compressed_path
                backup_filename = backup_path.name
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            size_mb = backup_path.stat().st_size / (1024 * 1024)
            
            logger.info(f"‚úÖ Backup created successfully: {backup_filename} ({size_mb:.2f} MB)")
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã (—Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π)
            import time
            current_time = time.time()
            for old_backup in backup_dir.glob("siteheater_backup_*.sql.gz*"):
                if old_backup.stat().st_mtime < current_time - (7 * 24 * 60 * 60):
                    old_backup.unlink()
                    logger.info(f"üóë Deleted old backup: {old_backup.name}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∞–º
            admins = await db_manager.get_all_admins()
            
            message = (
                f"üíæ <b>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±—ç–∫–∞–ø –≤—ã–ø–æ–ª–Ω–µ–Ω</b>\n\n"
                f"üìÅ –§–∞–π–ª: <code>{backup_filename}</code>\n"
                f"üì¶ –†–∞–∑–º–µ—Ä: {size_mb:.2f} MB\n"
                f"üïê –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                f"‚úÖ –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ"
            )
            
            for admin in admins:
                try:
                    await self.bot.send_message(admin.id, message, parse_mode="HTML")
                except Exception as e:
                    logger.warning(f"Failed to send backup notification to admin {admin.id}: {e}")
            
            logger.info(f"üì§ Backup notification sent to {len(admins)} admins")
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Backup timeout!")
        except Exception as e:
            logger.error(f"‚ùå Backup failed: {e}", exc_info=True)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∞–¥–º–∏–Ω–∞–º
            try:
                admins = await db_manager.get_all_admins()
                error_msg = (
                    f"‚ùå <b>–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –±—ç–∫–∞–ø–∞</b>\n\n"
                    f"‚ö†Ô∏è {str(e)[:200]}\n\n"
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞."
                )
                
                for admin in admins:
                    try:
                        await self.bot.send_message(admin.id, error_msg, parse_mode="HTML")
                    except: pass
            except:
                pass


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
warming_scheduler = WarmingScheduler()

